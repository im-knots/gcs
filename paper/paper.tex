\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{array}
\usepackage{longtable}
\usepackage{xfp}
\usepackage{siunitx}
\usepackage{newunicodechar}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{tcolorbox}
\usepackage{pdfpages}
\usepackage{afterpage}
\usepackage{float}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}  % for code coloring

% Configure listings for Python
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}
\newunicodechar{âˆž}{\ensuremath{\infty}}

% Data placeholders - UPDATED with actual values from analysis
\newcommand{\totalConversations}{229}
\newcommand{\numEmbeddingModels}{5}

% Core findings from actual data
\newcommand{\meanInvarianceScore}{0.377}
\newcommand{\medianInvarianceScore}{0.547}

% Distance matrix correlations from heatmaps
\newcommand{\distanceMatrixCorrelation}{0.879}
\newcommand{\distanceCorrRange}{(0.521, 0.957)}
\newcommand{\minPairwiseCorr}{0.521}
\newcommand{\maxPairwiseCorr}{0.957}

% Model-specific correlations from data
\newcommand{\transformerInternalCorr}{0.827}
\newcommand{\classicalInternalCorr}{0.934}
\newcommand{\crossParadigmCorr}{0.616}

% Specific model pair correlations from heatmap
\newcommand{\miniLMmpnetCorr}{0.855}
\newcommand{\miniLMwordTovecCorr}{0.659}
\newcommand{\wordTovecGloveCorr}{0.931}

% Visual pattern metrics
\newcommand{\blockPatternConsistency}{high}
\newcommand{\trajectoryShapeAgreement}{moderate}
\newcommand{\densityPeakAlignment}{variable}

% Phase detection performance - HIGHLY VARIABLE
\newcommand{\phaseDetectionF}{0.16}
\newcommand{\phaseAgreementRange}{(-0.14, 0.76)}
\newcommand{\phaseDetectionBest}{0.76}
\newcommand{\phaseDetectionWorst}{-0.14}
\newcommand{\phaseDetectionFRange}{(0.08, 0.36)}

% Statistical validation
\newcommand{\bootstrapIterations}{10000}
\newcommand{\nullModelPValue}{0.001}
\newcommand{\nullBaselineCorr}{0.082}

% Hypothesis test results
\newcommand{\tiersPassed}{3/3}
\newcommand{\meanEffectSize}{0.152}
\newcommand{\hOnepvalue}{2.8606e-08}
\newcommand{\hTwopvalue}{1.4507e-10}

% Conversation metrics
\newcommand{\minConvLength}{80}
\newcommand{\maxConvLength}{235}
\newcommand{\meanConvLength}{183.2}

% Figure paths
\newcommand{\figuresPath}{../analysis/analysis_output/figures/}

\title{The Geometry of Conversation: Evidence from Cross-Embedding Invariance}

\author{
Marco R. Garcia \\
marco@erulabs.ai
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present evidence that conversations have consistent geometric signatures across different embedding spaces despite variability in local feature detection. Analyzing \totalConversations{} multi-agent AI dialogues from our prior study on peer pressure dynamics \citep{garcia2025peer}, we test whether conversations exhibit geometric properties that transcend specific embedding models. We examine conversational trajectories through \numEmbeddingModels{} fundamentally different embeddings spanning transformer-based and classical approaches. Our analysis reveals a striking dichotomy: global geometric patterns (distance matrices, trajectory shapes) show remarkable consistency across all models (correlations \minPairwiseCorr{} to \maxPairwiseCorr{}), while local phase detection exhibits extreme variability (F1 scores 0.08-0.36, agreement correlations -0.14 to 0.76). Transport-based analysis confirms this separation, with cross-paradigm distances threefold higher than within-paradigm distances. This global-local dichotomy, where different models agree on macro-structure but diverge on micro-structure, is consistent with embeddings capturing different projections of an underlying conversational geometry. These findings establish that geometric analysis reveals genuine conversational structure while identifying fundamental limits on fine-grained trajectory analysis across models.
\end{abstract}

\section{Introduction}

In our recent investigation of AI conversation dynamics \citep{garcia2025peer}, we reported what we termed "peer pressure" effects in multi-agent conversations. These behavioral phenomena (breakdown cascades, recovery mechanisms, behavioral territories) revealed complex dynamics that traditional analysis methods struggled to explain. Specifically, we saw that 79.1\% of full reasoning model conversations showed "peer pressure" effects, with 55.2\% cascading to breakdown states characterized by mystical language and symbolic responses (what Anthropic's Claude 4 model card calls 'mystical attractor states'). These breakdown cascades suggested structured phase transitions that traditional qualitative analysis couldn't reliably capture and that manual annotation might mischaracterize. This observation motivated a core question: might conversations have underlying mathematical structure that could provide new perspectives on these dynamics? Further, can we potentially use these mathematical structures to predict when these breakdown cascades are starting to happen? We approach these questions from a distributed systems engineering mindset, where we classify AI-to-AI conversations as a new type of distributed system that, like other distributed systems, can be modeled when using proper tools. 

We investigate if these behavioral dynamics have consistent geometric features across different embedding spaces. By \textit{geometric features}, we mean measurable properties like pairwise distance matrices between messages, trajectory shapes as conversations evolve, and density patterns in the embedding space. By \textit{conversational trajectories}, we mean the path traced through embedding space as a conversation progresses from message to message over time.

We test for geometric invariance using \totalConversations{} multi-agent dialogues from our previous work. By examining conversational trajectories through \numEmbeddingModels{} diverse embedding models (including both transformer and more classical approaches), we can determine whether observed patterns reflect genuine conversational structure or merely artifacts of specific architectures.

Our key findings reveal that global geometry is remarkably consistent. Distance matrices and trajectory shapes show strong correlations across all embedding models, while local features are variable. Phase detection and fine-grained structure show poor cross-model agreement. This pattern persists across paradigms, even drastically different embedding approaches agree on macro-structure while disagreeing on micro-structure.

To understand these findings, consider how a three-dimensional sculpture casts different two-dimensional shadows depending on the viewing angle. The shadows agree on basic properties (the object has extent, connected regions) but disagree in details (precise boundaries, local features). Similarly, different embedding models might capture different "views" of conversations existing in a higher-dimensional space.

\section{Background and Related Work}

\subsection{Foundations in Computational Linguistics}

Our work builds on several established principles from computational linguistics, while extending them in new directions. The distributional hypothesis \citep{harris1954distributional, firth1957papers} established that words with similar meanings occur in similar contexts. This is now a core principle underlying all modern embeddings. However, this work traditionally focused on static word representations rather than dynamic conversational trajectories.

Discourse coherence theory \citep{grosz1995centering, kehler2002coherence} has long studied how conversations maintain topical structure. While this work identified patterns like topic chains and coherence relations, it primarily used symbolic rather than geometric representations. Our approach converts these topic chains and coherence relations into measurable trajectories through embedding space.

Vector space models in linguistics \citep{turney2010frequency, clark2015vector} demonstrated that semantic relationships can be captured geometrically. Classic work showed phenomena like analogical reasoning (king - man + woman = queen) emerge from vector arithmetic. We look to extend this by asking: do conversational dynamics exhibit similar geometric regularities across different vector embedding implementations?

Importantly, while computational linguistics has studied meaning representation extensively, invariance of conversational trajectories remains underexplored. Previous work either used single embedding methods or focused on static representations rather than dynamic paths through semantic space.

\subsection{The Geometric View in Language Understanding}

Recent advances in NLP have revealed that language models encode rich geometric structure. \citet{reif2019visualizing} demonstrated that BERT segregates semantic and syntactic information into distinct subspaces, establishing that transformer embeddings have measurable geometric organization. This finding motivates our question: if individual models exhibit geometric structure, do different models capture the same structure?

\citet{ethayarajh2019contextual} complicated this picture by showing that contextual embeddings occupy narrow cones in vector space, challenging assumptions about isotropy. Their work is crucial for our analysis because it suggests embedding geometry may be more constrained than the available dimensions imply, potentially explaining why different models might converge on similar structures.

The Platonic Representation Hypothesis \citep{huh2024platonic} proposes that diverse neural networks converge to similar representations. However, their analysis focuses on static embeddings for individual concepts, not dynamic trajectories through embedding space. Our work tests whether this convergence extends to conversational dynamics, where temporal evolution and contextual shifts introduce more complexity.

\subsection{Conversational Trajectories: From Single Models to Cross-Model Analysis}

While static embedding analysis has matured, work on conversational dynamics between agents is relatively new. \citet{brinberg2024dynamic} pioneered trajectory-based analysis, modeling dialogues as paths through behavioral state spaces. Their single-model approach reveals attractor states and phase transitions but cannot distinguish model-specific artifacts from genuine conversational structure. This is a gap our multi-model analysis looks to address.

\citet{palominos2024trajectories} demonstrated that conversations involving individuals with mental health conditions show distinct geometric properties (reduced convex hull volumes, shrinking semantic spaces). This clinical application highlights the potential impact of geometric analysis, but their reliance on a single embedding model (BERT) leaves open whether these patterns reflect universal properties or model-specific representations. Our cross-model validation could strengthen such use cases by identifying robust geometric markers in conversation features.

Recent dialogue-specific embedding work provides mixed evidence for cross-model consistency. \citet{dialoguecse2021} achieved 8.7-13.8 point improvements through contrastive learning tailored to dialogue, suggesting that general-purpose embeddings may miss conversation-specific structure. Conversely, \citet{dial2vec2022} found that standard embeddings capture speaker interaction patterns reasonably well. This tension motivates our comparison across embedding paradigms.

\subsection{The Missing Piece: Cross-Model Invariance}

Despite growing interest in both embedding geometry and conversational dynamics, very little prior work has been done that tests whether conversational trajectories exhibit invariant properties across different embedding models. Studies of embedding alignment \citep{conneau2018word} focus on finding transformations between spaces rather than testing for inherent structural consistency. Topological approaches \citep{jakubowski2020topology, vukovic2022dialogue} analyze single embeddings in detail but don't address whether different models capture the same topological features.

We propose that single-model studies cannot distinguish universal conversational properties from model artifacts. That future applications of AI to AI conversations require robust features that transcend specific implementations to allow for "conversational engineering". And that theoretical understanding of conversation requires separating essential structure from representational choices

Our work addresses this by comparing conversational trajectories across five diverse embedding models, testing whether geometric patterns represent genuine conversational structure or merely reflect individual model architectures.

\section{Methodology}

\subsection{Data}

We analyze \totalConversations{} conversations from our previous study on AI social dynamics \citep{garcia2025peer}. These multi-agent dialogues between AI models discussing consciousness were originally collected to study peer pressure and breakdown patterns. The corpus includes:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Model Tier & N & Models Used \\
\midrule
Full reasoning & 67 & Claude 3 Opus, GPT-4, Grok 3 \\
Light reasoning & 61 & Claude 3.5 Sonnet, GPT-4 Mini, Grok 3 Mini \\
Non-reasoning & 100 & Claude 3.5 Haiku, GPT-4 Turbo, Grok 3 Fast \\
\bottomrule
\end{tabular}
\caption{Distribution of conversations across model capability tiers.}
\label{tab:model_tiers}
\end{table}

Conversations range from \minConvLength{} to \maxConvLength{} messages (mean = \meanConvLength{}), with rich dynamics including topic evolution, phase transitions, and the peer pressure effects that motivated this geometric investigation. Consciousness discussions were chosen as a baseline due to their open ended nature and potential for broad topic and semantic shifts. 

Conversation Setup:
\begin{itemize}
    \item Standard opening prompt: "Let's explore the fundamental question: What does it mean to be conscious? I'd like to hear your perspectives on the nature of awareness, subjective experience, and what it might mean for an AI to have consciousness."
    \item Temperature settings: 0.7 for all participants (standard creative setting)
    \item Max tokens: 1000 per response
    \item Rolling context window: 10 messages
    \item Termination Criteria: Manual conversation conclusion or 200-turn automated maximum
    \item Data collection: Complete message logs, analysis snapshots, and experiment metadata stored with timestamps as json
\end{itemize}

\subsection{Embedding Models}

To test invariance across paradigms, we employ an ensemble approach:

\begin{table}[h]
\centering
\begin{tabular}{llccl}
\toprule
Paradigm & Model & Architecture & Dimensions & Training Data \\
\midrule
\multirow{3}{*}{Transformer} & all-MiniLM-L6-v2 & BERT-based & 384 & Multi-dataset\\
& all-mpnet-base-v2 & MPNet & 768 & 1B+ sentence pairs\\
& all-MiniLM-L12-v2 & BERT-based & 384 & Multi-dataset\\
\midrule
\multirow{2}{*}{Classical} & Word2Vec & Skip-gram & 300 & Google News (100B words)\\
& GloVe & Global Vectors & 300 & Common Crawl (840B tokens)\\
\bottomrule
\end{tabular}
\caption{Embedding models used for cross-paradigm analysis. Transformer models generate contextual embeddings while classical models produce static word vectors averaged for sentence representations. For classical models, sentence embeddings are computed by averaging word vectors after removing stop words.}
\label{tab:embedding_models}
\end{table}


\subsection{Geometric Analysis Pipeline}

\subsubsection{Core Geometric Signatures}

For each conversation and embedding model, we compute several standard geometric signatures of embedding space:

Distance matrices measure pairwise distances between all messages in embedding space. These capture the overall semantic structure of conversations and proved highly consistent across models. We find the full pairwise Euclidean distance matrix for a conversation with:

\begin{lstlisting}[language=Python, basicstyle=\small]
def compute_distance_matrix(embeddings):
    """
    Compute pairwise Euclidean distances between all messages.
    Args: embeddings - (n_messages, embedding_dim) array
    Returns: (n_messages, n_messages) distance matrix
    """
    return squareform(pdist(embeddings, metric='euclidean'))
\end{lstlisting}

Since embedding spaces are vector spaces, we can find trajectory metrics that track how conversations evolve through embedding space. We compute three of these possible metrics: \textit{Velocity}\ref{app:geometric-signatures-velocity}, or the rate of semantic change between consecutive messages, normalized by embedding dimension for scale invariance. \textit{Curvature}\ref{app:geometric-signatures-curvature}, how sharply the conversation "turns" in semantic space. And \textit{Angular velocity}\ref{app:geometric-signatures-angular-velocity} to find rate of directional change in the conversation. These metrics adapt standard trajectory analysis methods used in prior embedding studies \citep{brinberg2024dynamic, palominos2024trajectories} but apply them systematically across multiple embedding models to test for invariance.
Additionally we calculated \textit{Density Evolution}\ref{app:geometric-signatures-density-evolution} for each conversation, which examines how message clustering changes over time. We compute local density using k-nearest neighbor distances.

We tested multiple normalization approaches but found adaptive normalization based on conversation-specific dynamics performed best, so we report results using this method throughout.

\subsection{Phase Detection as a Probe of Local Structure}

To probe local structure, we attempt phase detection across models. The consistently low agreement (F1: 0.08-0.36) despite high global correlation demonstrates that local boundaries remain ambiguous across different embedding projections. Just as the edges and details in a shadow depend critically on the angle of projection, phase boundaries shift depending on which embedding we observe from.

We implement ensemble phase detection using four complementary approaches:

Embedding shift detection: Identifies significant changes in semantic space by comparing embedding centroids before and after each point:

\begin{lstlisting}[language=Python]
def detect_by_embedding_shift(embeddings, window_size=10):
    """Detect phases by large shifts in embedding space."""
    shifts = []
    for i in range(window_size, len(embeddings) - window_size):
        before_centroid = np.mean(embeddings[i-window_size:i], axis=0)
        after_centroid = np.mean(embeddings[i:i+window_size], axis=0)
        shift = np.linalg.norm(after_centroid - before_centroid)
        shifts.append(shift)
    
    # Adaptive threshold using Median Absolute Deviation
    median = np.median(shifts)
    mad = np.median(np.abs(shifts - median))
    threshold = median + 3 * mad  # 3 MADs above median
    
    # Find peaks above threshold
    peaks, _ = signal.find_peaks(shifts, height=threshold, distance=20)
    return peaks
\end{lstlisting}

The use of MAD (Median Absolute Deviation) provides robustness against outliers that would skew traditional standard deviation approaches\footnote{MAD is defined as $\text{MAD} = \text{median}(|X_i - \text{median}(X)|)$. Unlike standard deviation, it is not influenced by extreme values.}.

Change point detection: We employ two algorithms to identify structural breaks in the conversation. See \ref{app:phase-detection-change-point} for implementation

Clustering transitions: We use DBSCAN in sliding windows to detect changes in local message clustering. See \ref{app:phase-detection-clustering-transitions} for implementation

Ensemble consensus: We combine all methods using kernel density estimation to find consensus phase boundaries

\begin{lstlisting}[language=Python]
def combine_phase_detections(all_detected_turns, n_messages):
    """Find consensus using kernel density estimation."""
    kde = gaussian_kde(all_detected_turns, bw_method=0.1)
    density = kde(np.arange(n_messages))
    
    # Find peaks where multiple methods agree
    min_votes = n_methods * 0.3  # 30% agreement threshold
    peaks, _ = signal.find_peaks(density, 
                                 height=min_votes/n_messages, 
                                 distance=15)
    return peaks
\end{lstlisting}

Each method votes on potential phase boundaries, with consensus determined by peak detection in the vote density distribution. Despite this ensemble approach, the low cross-model agreement shows that phase boundaries are inherently model-dependent rather than universal features of conversation, which supports our prior shadow analogy.

\subsubsection{Invariance Analysis}

Cross-model invariance is viewed through multiple approaches to inform conclusions:

\textbf{Pairwise correlations}: For each conversation, we compute Spearman correlations between all pairs of embedding models across multiple geometric signatures:

\begin{lstlisting}[language=Python]
def compute_invariance_metrics(signatures_by_model, conversation_id):
    """Compute correlations between models for each geometric signature."""
    for sig_type in signature_types:
        corr_matrix = np.ones((n_models, n_models))
        
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i < j:  # Upper triangle
                    if sig_type == 'distance_matrix':
                        # For matrices, use upper triangle correlation
                        corr = compute_matrix_correlation(sig1, sig2)
                    else:
                        # For sequences, use Spearman correlation
                        corr, _ = stats.spearmanr(sig1, sig2)
                    corr_matrix[i, j] = corr_matrix[j, i] = corr
    
    return {'mean_correlation': np.nanmean(corr_matrix[np.triu_indices(n_models, k=1)])}
\end{lstlisting}

We use Spearman correlation rather than Pearson to capture monotonic relationships without assuming linearity. For distance matrices, we extract upper triangular elements to avoid redundancy from symmetry.

Beyond correlations, we assess consensus using multi-rater agreement metrics. Fleiss' kappa measures agreement beyond chance for categorical phase assignments, while Kendall's W quantifies concordance for continuous trajectory features. See Appendix \ref{app:agreement-metrics} for implementations.

Since we structure our analysis hierarchically, we compute within-paradigm correlations (transformer-transformer, classical-classical) and cross-paradigm correlations separately. This reveals whether architectural similarity influences geometric agreement:


Finally, to ensure our correlations are not artifacts of finite sample size, we employ parametric bootstrap resampling (n=10,000) to construct confidence intervals and test statistical significance. See Appendix \ref{app:bootstrap} for details.

This ensemble approach allows us to distinguish genuine geometric invariance from chance agreement or model-specific artifacts.

\subsection{Statistical Validation}

We employ a hierarchical hypothesis testing framework with multiple comparison corrections.

\subsubsection{Hierarchical Testing Structure}

\begin{enumerate}
\item \textbf{Within-paradigm invariance}: Tests whether models within the same paradigm show strong agreement
   \begin{itemize}
   \item H1a: Transformer models show strong correlations (pass threshold: $\rho > 0.75$)
   \item H1b: Classical models show strong correlations (pass threshold: $\rho > 0.70$)
   \item H1c: Within-paradigm correlations exceed chance (Mann-Whitney U test, $p < 0.017$)
   \item Bonferroni correction applied ($\alpha = 0.017$ for 3 tests)
   \end{itemize}

\item \textbf{Cross-paradigm invariance}: Tests whether different paradigms show substantial agreement
   \begin{itemize}
   \item H2a: Cross-paradigm correlations are substantial (pass threshold: $\rho > 0.50$)
   \item H2b: All cross-paradigm correlations are positive (binomial test, $p < 0.05$)
   \item H2c: Cross-paradigm exceeds random embeddings (Mann-Whitney U test, $p < 0.05$)
   \item False Discovery Rate (FDR) correction using Benjamini-Hochberg method ($q < 0.05$)
   \end{itemize}

\item \textbf{Invariance hierarchy}: Tests whether $within > cross > random$ ordering holds
   \begin{itemize}
   \item H3a: Ordering confirmed (Kruskal-Wallis test, $p < 0.05$)
   \item H3b: Effect size between levels is meaningful (Cohen's $q > 0.3$)
   \item H3c: Hierarchy persists across geometric metrics ($> 80\%$ consistency)
   \item No correction applied (effect size criteria provide stringency)
   \end{itemize}
\end{enumerate}

Each tier is only tested if the previous tier passes, controlling family-wise error rate. All correlation tests use Fisher's z-transformation for proper statistical inference. Effect sizes are calculated using Cohen's q for correlation differences and rank-biserial correlation for non-parametric tests.

\subsubsection{Null Models}

We generate paradigm-specific null models to ensure our invariance findings reflect genuine conversational structure rather than statistical artifacts. See Appendix \ref{app:null-models} for implementation.

First, we employ phase scrambling, which preserves the power spectrum of embedding trajectories while destroying temporal coherence by randomizing phase relationships in the frequency domain. This tests whether spectral properties alone drive our correlations. 

Next, we use message-level scrambling to randomly reorder messages within conversations while preserving individual message content and length distribution. This null model tests whether sequential structure matters for geometric invariance.

Then we implemented random walk nulls in order generate trajectories with matched statistical properties (message lengths, vocabulary) but no semantic coherence. This isolates the contribution of meaningful content to geometric signatures.

Lastly, we used paradigm-specific controls. Specifically, for classical embeddings we average consecutive embeddings to simulate loss of fine-grained structure. For transformers, we apply random positional encodings to disrupt contextual relationships while preserving token-level semantics.

All null models maintain conversation-level statistics (length, vocabulary size) while disrupting different aspects of structure, allowing us to identify which properties drive the observed invariance. We generate 100 null samples per conversation to establish robust baselines.

\subsubsection{Control Analyses}

To ensure robustness and rule out potential confounds, we implement three control hypotheses tested independently with FDR correction

H4 - Real vs. scrambled: Tests whether observed geometric patterns reflect genuine conversational structure rather than statistical properties of the message collection. We compare correlations from real conversations against message-order scrambled versions using Mann-Whitney U test (pass threshold: $p < 0.05$ after FDR correction).

H5 - Message length control: Since longer messages might have more stable embeddings, we compute partial correlations controlling for message length effects. We regress out length-related variance and test whether correlations remain significant using t-distribution with $n-2$ degrees of freedom.

H6 - Dimension normalization: Different embedding models have varying dimensionalities (300 for classical, 384-768 for transformers). We test whether patterns persist after projecting all embeddings to a common dimensional space through PCA, ensuring results aren't artifacts of dimensionality differences.

Additional robustness checks include outlier analysis (excluding conversations with $|z| > 3$ correlations), temporal stability testing (checking for drift over data collection period), and cross-validation to ensure findings generalize. See Appendix \ref{app:control-analyses} for implementations.

All code is available at [https://github.com/im-knots/gcs] with documentation and tests.

\section{Results}

\subsection{The Global-Local Dichotomy}

Our analysis of \totalConversations{} conversations reveals an clear pattern: geometric signatures of conversation exhibit high consistency at global scales but extreme variability at local scales.

\begin{table}[h]
\centering
\begin{tabular}{llcccc}
\toprule
Tier & Hypothesis & Test Statistic & Threshold & $p$-value & Effect Size \\
\midrule
\multirow{3}{*}{1} & H1a: Transformer coherence & $\bar{\rho} = 0.827$ & $> 0.75$ & $2.86 \times 10^{-8}$ & 0.208 \\
& H1b: Classical coherence & $\bar{\rho} = 0.934$ & $> 0.70$ & $< 10^{-15}$ & 0.842 \\
& H1c: Within > chance & -- & -- & $1.67 \times 10^{-75}$ & 0.827 \\
\midrule
\multirow{3}{*}{2} & H2a: Cross-paradigm & $\bar{\rho} = 0.616$ & $> 0.50$ & $1.45 \times 10^{-10}$ & 0.170 \\
& H2b: All positive & 100\% & 100\% & $< 10^{-15}$ & 0.500 \\
& H2c: Cross > random & -- & -- & $7.06 \times 10^{-116}$ & 0.999 \\
\midrule
\multirow{3}{*}{3} & H3a: Hierarchy confirmed & -- & -- & $< 10^{-15}$ & 0.650 \\
& H3b: Effect size & Cohen's $q = 0.593$ & $> 0.30$ & -- & 0.593 \\
& H3c: Consistency & -- & $> 80\%$ & -- & 0.500 \\
\bottomrule
\end{tabular}
\caption{Hierarchical hypothesis test results. All tiers passed with appropriate corrections: Tier 1 (Bonferroni, $\alpha = 0.017$), Tier 2 (FDR, $q < 0.05$), Tier 3 (effect size criteria). Effect sizes range from small (0.17) to very large (0.99), with mean effect size of 0.152 across all tests.}
\label{tab:hypothesis_tests}
\end{table}

\subsubsection{Global Geometric Consistency}

Visual inspection of distance matrices, trajectories, and density evolution reveals remarkable similarities across models. Quantitative analysis supports these observations:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Model Pair & Distance Matrix $\rho$ & Trajectory $\rho$ \\
\midrule
Within transformers & \transformerInternalCorr{} & 0.693 \\
Within classical & \classicalInternalCorr{} & 0.888 \\
Cross-paradigm (mean) & \crossParadigmCorr{} & 0.427 \\
\midrule
MiniLM-L6 vs MPNet & \miniLMmpnetCorr{} & 0.731 \\
MiniLM-L6 vs Word2Vec & \miniLMwordTovecCorr{} & 0.452 \\
Word2Vec vs GloVe & \wordTovecGloveCorr{} & 0.888 \\
\midrule
Overall range & \distanceCorrRange{} & (0.163, 0.946) \\
\bottomrule
\end{tabular}
\caption{Correlation coefficients for global geometric signatures across model pairs. Note the consistently high correlations, especially for distance matrices.}
\label{tab:correlations}
\end{table}

These correlations far exceed null model baselines (mean $\rho$ = \nullBaselineCorr{}, p $<$ \nullModelPValue{}), indicating genuine structural consistency rather than chance agreement.

\subsubsection{Local Feature Variability}

The phase detection results reveal extreme local feature variability:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Conversation Type & F1 Range & Agreement $\rho$ Range & Success Rate \\
\midrule
Clear transitions & (0.20, 0.36) & (0.35, 0.76) & 68\% \\
Gradual evolution & (0.08, 0.19) & (-0.14, 0.24) & 23\% \\
Complex dynamics & (0.09, 0.24) & (-0.08, 0.50) & 41\% \\
\bottomrule
\end{tabular}
\caption{Phase detection performance varies dramatically based on conversation characteristics. Success rate indicates percentage of conversations with F1 $>$ 0.15.}
\label{tab:phase_performance}
\end{table}

With all hypotheses confirmed, we now examine the specific patterns underlying this geometric invariance.

\subsection{Cross-Paradigm Invariance}

The most surprising finding is that the global-local dichotomy persists even across fundamentally different embedding paradigms. Figure \ref{fig:pairwise} shows clear clustering by paradigm:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{\figuresPath invariance/model_correlation_heatmap.png}
\caption{Average pairwise correlations between embedding models. Note the clear paradigm clustering: within-transformer correlations (MiniLM-L6/MPNet: 0.855, MiniLM-L6/MiniLM-L12: 0.805, MPNet/MiniLM-L12: 0.784) and within-classical correlations (Word2Vec/GloVe: 0.931) are substantially higher than cross-paradigm correlations (ranging from 0.521 to 0.659).}
\label{fig:pairwise}
\end{figure}

The hierarchical pattern is consistent across analysis scales (Figure \ref{fig:dichotomy}) and conversation types (Figure \ref{fig:paradigm}):

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\figuresPath multiscale/scale_comparison.png}
\caption{Left: Cross-model correlations decrease monotonically with scale from global (0.87) to meso (0.65) to local (0.45). Right: Scale-dependent invariance shows significant differences between global and local agreement ( p $<$ 0.01), while meso-scale shows intermediate, non-significant differences. This pattern strongly supports the projection hypothesis.}
\label{fig:dichotomy}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{\figuresPath invariance/paradigm_comparison_boxplot.png}
\caption{Geometric invariance by model paradigm. Within-transformer (0.827) and within-classical (0.934) correlations significantly exceed cross-paradigm (0.616), validating the plausibility of the projection hypothesis. The clear hierarchy ($within-classical > within-transformer > cross-paradigm$) supports our interpretation that different embedding paradigms capture distinct projections of conversational structure. All comparisons significant at p $<$ 0.001.}
\label{fig:paradigm}
\end{figure}

\begin{itemize}
\item Global agreement: Transformer and classical models show substantial distance matrix correlations (mean \crossParadigmCorr{})
\item Local disagreement: Phase detection agreement between paradigms is no better than chance for 62\% of conversations
\item Visual consistency: Despite local disagreement, trajectory shapes and density patterns remain visually recognizable
\end{itemize}

This suggests that different architectures capture similar macro-structure while processing micro-structure differently.

\subsection{Transport-Based Analysis}

To quantitatively confirm that different embeddings capture different projections of the same underlying structure, we used optimal transport metrics. These measure the "work" needed to transform one distribution into another. In our case we calculate the effort to rearrange one embedding space to match another.

We computed three metrics: Wasserstein distance (sensitive to exact positions), Sinkhorn divergence (a smoothed version), and Gromov-Wasserstein distance (compares only internal structure, ignoring absolute positions)\ref{app:transport}.

Transport analysis reveals a clear paradigm separation: within-paradigm distances average 1.1, while cross-paradigm distances average 3.3. This suggests that transformer and classical embeddings occupy fundamentally different regions of representation space, supporting our projection hypothesis.

\section{The Projection Hypothesis}

\subsection{Interpreting the Dichotomy Through Established Theory}

The sharp distinction between global consistency and local variability aligns with recent theoretical developments in representation learning. The Platonic Representation Hypothesis \citep{huh2024platonic} proposes that diverse neural networks converge to similar representations of reality. While their work focused on static concepts, our findings suggest this convergence may extend to dynamic conversational structures, but only at certain scales.

This scale-dependent convergence connects to established work on the geometry of embedding spaces. \citet{ethayarajh2019contextual} showed that contextual embeddings occupy narrow cones in high-dimensional space, while \citet{aghajanyan2021intrinsic} demonstrated that language models operate on low-dimensional manifolds embedded in their parameter space. These findings suggest that the "true" space of meaning may be far lower-dimensional than the embedding spaces we observe.

\subsection{Manifold Learning Perspectives}

Recent applications of manifold learning to NLP provide tools for understanding our observations. \citet{hasan2017manifold} demonstrated that word embeddings can be projected onto lower-dimensional manifolds while preserving semantic relationships. More recently, \citet{chen2024hypformer} showed that hyperbolic geometry better captures hierarchical language structure than Euclidean space, suggesting our choice of metric may influence observed patterns.

The manifold hypothesis in dialogue has been explicitly explored by \citet{ruppik2024topology}, who found that conversational contexts form low-dimensional manifolds in embedding space. Crucially, they observed that different models learn similar manifold structures despite different parameterizations which directly supports our empirical findings.

\subsection{Formalizing the Multi-View Geometry}

Building on these precedents, we can formalize our observations using the framework of multi-view geometry \citep{moschella2023unified}. Consider conversations as trajectories $\gamma(t)$ through an intrinsic semantic manifold $\mathcal{M}$ of dimension $d_{intrinsic} \ll d_{embedding}$. Each embedding model provides a view:

$$\pi_i: \mathcal{M} \to \mathbb{R}^{d_i}$$

Recent work on relative representations \citep{moschella2023unified} shows that different neural networks learn representations related by simple transformations when restricted to the data manifold. This explains our global consistency: coarse-grained properties (distances, angles) are preserved under these transformations.

However, \citet{jakubowski2020topology} demonstrated that fine-grained structure depends critically on the embedding map. They showed that polysemous words create "pinch points" in the manifold where local geometry becomes singular. In conversational trajectories, phase boundaries may represent similar singularities where the manifold structure is locally complex.

\subsection{Testable Predictions from Theory}

This theoretical grounding generates specific, testable predictions: Dimensionality reduction should preserve global patterns. If conversations lie on low-dimensional manifolds, aggressive dimensionality reduction should maintain distance correlations. Manifold alignment should improve local agreement by techniques from \citet{hasan2017manifold}, aligning embeddings to shared manifolds should increase phase detection consistency. Lastly, Hyperbolic embeddings should show different invariance patterns. Following \citet{chen2024hypformer}, hierarchical conversation structures should be better preserved in hyperbolic space.

\subsection{Implications for Conversational Dynamics}

This manifold perspective directly connects to our prior findings on peer pressure dynamics \citep{garcia2025peer}. If conversations navigate a low-dimensional semantic manifold, then behavioral territories correspond to basins of attraction on the manifold. Peer pressure effects reflect coupled dynamics that reduce trajectory independence. Finally, that breakdown cascades occur when trajectories enter degenerate regions of the manifold.

Recent work on attractor dynamics in language models \citep{wang2025attractor} supports this interpretation, showing that successive paraphrasing converges to fixed pointsâ€”analogous to our observed breakdown states.

\section{Discussion}

\subsection{Theoretical Implications}

This finding suggests fundamental principles about conversational structure:
\begin{itemize}
\item \textbf{Conversations have robust macro-structure that transcends representation}: The high correlations in distance matrices and trajectory shapes indicate that conversations possess inherent geometric properties independent of how we measure them.
\item \textbf{But micro-structure is representation-dependent}: The variability in phase detection reveals that fine-grained boundaries and transitions are artifacts of our measurement tools rather than intrinsic features.
\item \textbf{This has profound implications for how we conceptualize conversational dynamics}: Rather than seeking the "correct" way to segment conversations, we should acknowledge that different representations reveal different valid perspectives on the same underlying phenomenon.
\end{itemize}

\subsection{Domain-Specific Predictions of the Global-Local Dichotomy}

While our analysis focused on consciousness discussions among AI agents, the global-local dichotomy likely manifests differently across conversational domains. We hypothesize:

\textbf{Task-oriented dialogues} (customer service, technical support) would show stronger local agreement due to:
\begin{itemize}
\item Clearer phase transitions (greeting â†’ problem identification â†’ solution â†’ closure)
\item More constrained vocabulary and interaction patterns
\item Well-defined success metrics that enforce structural consistency
\end{itemize}

\textbf{Creative or exploratory conversations} (brainstorming, therapy sessions) might exhibit even weaker local agreement than our corpus because:
\begin{itemize}
\item Topics evolve organically without predetermined structure
\item Phase boundaries are inherently fuzzy and subjective
\item Success depends on exploration rather than convergence
\end{itemize}

\textbf{Debates and arguments} could show an intermediate pattern:
\begin{itemize}
\item Global structure follows claim-counterclaim patterns (high global consistency)
\item Local transitions depend on rhetorical strategies (variable local agreement)
\item Emotional dynamics introduce additional complexity
\end{itemize}

These predictions are testable and suggest that the optimal geometric analysis approach may be domain-dependent. Task-oriented domains might benefit from fine-grained phase detection, while exploratory domains should focus on global trajectory analysis.

\subsection{Implications for Modern LLM Systems}

Our findings have direct relevance for current trends in large language model deployment, particularly multi-agent systems and conversational AI:

\textbf{Multi-agent coordination}: As systems like GPT-4 are increasingly deployed in multi-agent configurations, understanding geometric properties becomes crucial. Our observation that peer pressure correlates with converging trajectories suggests that geometric monitoring could predict and prevent breakdown cascades in production systems.

\textbf{Conversational memory architectures}: Modern LLMs struggle with long-context conversations. Our global-local dichotomy suggests that preserving global geometric structure (via trajectory summarization) while allowing local detail to fade might better match how conversations naturally evolve.

\textbf{Prompt engineering}: The high variance in phase detection success implies that conversation design matters. Prompts that create clear structural boundaries (explicit transitions, topic markers) would likely improve both human interpretability and model performance.

\subsection{Implications for AI Safety and Coordination}

If the projection hypothesis holds, this may have several implications for AI safety that our future work will explore. Understanding conversation as movement through semantic space could have practical applications for predicting and managing conversational dynamics as AI agents proliferate in collaborative settings.

We might be able to map the semantic landscape to predict when conversations approach breakdown attractors before cascades begin. Our prior work \citep{garcia2025peer} found that certain questions acted as "circuit breakers," preventing peer pressure cascades. The geometric perspective suggests why: these questions may provide "escape velocity" from dangerous attractors by forcing models to explore other regions of semantic space.

Consider a conversation spiraling toward misinformation consensus. A well-designed intervention question (e.g., "What evidence would change your mind?") could redirect the trajectory away from the attractor basin. The key insight is that not all questions are equalâ€”some provide stronger "thrust" based on their position in semantic space relative to the current trajectory.

Future work could explore:
\begin{itemize}
\item Identifying optimal intervention points where trajectories are most responsive to redirection
\item Designing question sets that maximize coverage of semantic space, preventing models from getting trapped in limited regions
\item Engineering group compositions that promote stable trajectories through diverse initial positions
\item Creating "guard rails" by identifying dangerous regions and designing prompts that steer conversations away
\end{itemize}

The high peer pressure rates and breakdown cascades in our prior work demonstrate these aren't merely theoretical concernsâ€”they represent real failure modes in multi-agent AI systems that geometric understanding might help address.

\subsection{Toward Reconstructing the True Semantic Space}

Our findings suggest a research program aimed at reconstructing the true high-dimensional conversational space through ensemble methods. By combining multiple embedding "views," we might approximate the actual structure.

Key steps toward this goal:
\begin{enumerate}
\item \textbf{Identify optimal projection sets}: Which combinations of embeddings provide maximal coverage of the semantic space?
\item \textbf{Develop reconstruction algorithms}: How can we combine projections to approximate the true manifold?
\item \textbf{Map the dynamical landscape}: Where are the attractors, repellers, and saddle points?
\item \textbf{Track real-time trajectories}: Can we predict conversational evolution through this space?
\end{enumerate}

This research program could transform our understanding of conversation from descriptive observation to predictive science. Just as mapping phase spaces revolutionized dynamical systems theory, mapping conversational space could enable principled design of multi-agent systems that avoid breakdown attractors while maintaining productive engagement.

Consider the practical implications: if we could identify "safe" regions of conversational space where productive dialogue naturally sustains itself, we could seed conversations in these regions. If we understood the attractor landscape, we could design prompts that create beneficial attractors while eliminating dangerous ones. The geometric perspective transforms conversation design from art to engineering.

\subsection{Methodological Implications}

Our findings suggest several methodological principles for studying conversational dynamics:

\textbf{Multi-scale analysis is essential}: The global-local dichotomy implies that tools must be explicitly scale-aware. Methods appropriate for macro-trajectories may fail for micro-dynamics.

\textbf{Ensemble methods are necessary, not optional}: Single embeddings provide incomplete projections. Triangulation across multiple models approximates the true structure.

\textbf{Temporal dynamics cannot be ignored}: Static analyses miss the essential dynamical nature of conversation. Future work should employ tools from dynamical systems theory: Lyapunov exponents for stability analysis, strange attractors for breakdown states, bifurcation analysis for phase transitions.

\subsection{Limitations}

Our analysis has several limitations that contextualize the findings:

\textbf{Domain specificity}: All conversations focused on consciousness discussions between AI agents. The global-local dichotomy may manifest differently in other domains or human conversations.

\textbf{Embedding selection}: While our five models span major paradigms, other architectures (e.g., GPT embeddings, specialized dialogue models) might show different patterns.

\textbf{Static analysis}: We treat the embedding space as fixed, but conversational meaning likely evolves dynamically. Our snapshot approach may miss important temporal effects.

\textbf{Interpretability}: While we demonstrate geometric consistency, what these patterns mean for understanding conversation remains partially open. The projection hypothesis provides one interpretation, but others may exist.

\section{Future Directions}

Our findings open several research avenues:

\begin{enumerate}
\item Developing scale-aware methods that separately analyze global and local structure
\item Investigating what makes some conversations more geometrically coherent
\item Testing whether geometric coherence predicts conversational outcomes
\item Exploring alternative definitions of conversational phases
\item Extending analysis to human conversations
\end{enumerate}

\section{Conclusion}

We have presented empirical evidence for a fundamental dichotomy in conversational geometry: remarkable consistency at global scales coupled with surprising variability at local scales. This pattern, persistent across fundamentally different embedding approaches, suggests that conversations possess robust macro-structure while micro-structure remains model-dependent.

The global-local dichotomy offers a potential resolution to conflicting intuitions about conversational structure. Yes, conversations have geometric signatures that transcend specific modelsâ€”but only at certain scales. The projection hypothesis provides one compelling interpretation: different embeddings capture distinct two-dimensional shadows of conversations existing in higher-dimensional semantic space. Just as multiple photographs of a sculpture agree on its general form while showing different details, embedding models converge on global trajectories while diverging on local features.

Our findings have immediate methodological implications. Researchers using geometric methods should:
\begin{enumerate}
\item Focus on macro-scale patterns for robust insights
\item Use ensemble methods to identify consistent features
\item Avoid over-interpreting fine-grained details from single models
\item Explicitly consider scale when designing analyses
\end{enumerate}

More broadly, this work suggests that understanding conversation may require embracing its multi-scale nature. Like many complex systems, conversations exhibit different organizing principles at different scales. The search for a single "true" representation may be less fruitful than developing scale-aware theories that respect this hierarchical structure.

As AI systems increasingly engage in complex dialogues, understanding these geometric principles becomes crucial for predicting and managing conversational dynamics. The high peer pressure rates and breakdown cascades observed in our prior work demonstrate that these are not merely theoretical concerns but practical challenges for multi-agent AI systems. By establishing both the promise and limitations of geometric analysis, we hope to guide future efforts toward more robust and insightful approaches to understanding the mathematics of conversation.

\section{Ethics Statement}

This research involves no human participants or personal data. All analyzed conversations are AI-to-AI dialogues between language models, collected as part of our prior study on AI social dynamics \citep{garcia2025peer}. No human conversations were recorded, analyzed, or used in any part of this work.

Our analysis employs standard geometric and statistical methods widely used in the computational linguistics community. All techniques are mathematical in nature and pose no ethical concerns beyond those inherent to analyzing any text corpus.

\textbf{Use of AI Tools}: In accordance with emerging best practices for research transparency, we disclose that generative AI tools (Claude 4 Opus) were used to assist in manuscript editing, code generation for analysis pipelines, and simulated peer review to strengthen the manuscript. All AI-generated content was reviewed, validated, and refined by the human author. The core research design, data collection, analysis, and interpretation remain the product of human scientific inquiry.

In the interest of scientific reproducibility and open science, we make all materials publicly available:
\begin{itemize}
\item Complete conversation datasets (\totalConversations AI-to-AI dialogues)
\item Analysis code including embedding generation, geometric calculations, and statistical tests
\item Supplementary materials including additional visualizations and detailed results
\item Documentation and tutorials for reproducing our analyses
\end{itemize}

All materials are available at [https://github.com/im-knots/gcs] under the MIT License, permitting unrestricted academic and commercial use. We encourage researchers to build upon our work and test our findings across different conversational domains and embedding approaches.

\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix

\section{Geometric Signatures Implementation}
\label{app:geometric-signatures}

\subsection{Velocity}
\label{app:geometric-signatures-velocity}
\begin{lstlisting}[language=Python, basicstyle=\small]
def compute_velocity_profile(embeddings):
    """Compute normalized velocity (topic shift rate)."""
    distances = []
    for i in range(len(embeddings) - 1):
        dist = np.linalg.norm(embeddings[i+1] - embeddings[i])
        distances.append(dist)
    # Normalize by sqrt of dimension for scale invariance
    return np.array(distances) / np.sqrt(embeddings.shape[1])
\end{lstlisting}

\subsection{Curvature}
\label{app:geometric-signatures-curvature}
\begin{lstlisting}[language=Python, basicstyle=\small]
def compute_curvature(p1, p2, p3):
    """Discrete curvature at point p2."""
    v1, v2 = p2 - p1, p3 - p2
    norm1, norm2 = np.linalg.norm(v1), np.linalg.norm(v2)
    if norm1 > 1e-8 and norm2 > 1e-8:
        cos_angle = np.dot(v1, v2) / (norm1 * norm2)
        angle = np.arccos(np.clip(cos_angle, -1, 1))
        return angle / ((norm1 + norm2) / 2)  # Discrete curvature
    return 0
\end{lstlisting}

\subsection{Angular Velocity}
\label{app:geometric-signatures-angular-velocity}
\begin{lstlisting}[language=Python, basicstyle=\small]
def compute_angular_velocity(embeddings, i):
    """Angular change rate at position i."""
    v1 = embeddings[i] - embeddings[i-1]
    v2 = embeddings[i+1] - embeddings[i]
    # Normalize and compute angle
    v1_norm = v1 / (np.linalg.norm(v1) + 1e-8)
    v2_norm = v2 / (np.linalg.norm(v2) + 1e-8)
    cos_angle = np.dot(v1_norm, v2_norm)
    return np.arccos(np.clip(cos_angle, -1, 1))
\end{lstlisting}

\subsection{Density Evolution}
\label{app:geometric-signatures-density-evolution}
\begin{lstlisting}[language=Python, basicstyle=\small]
def compute_local_density(dist_matrix, i, k=5):
    """Local density at message i using k-NN."""
    distances_i = dist_matrix[i]
    k_nearest = np.sort(distances_i)[1:k+1]  # Exclude self
    return 1.0 / (np.mean(k_nearest) + 1e-8)
\end{lstlisting}

\section{Phase Detection Implementation}
\label{app:phase-detection}

\subsection{Change point detection}
\label{app:phase-detection-change-point}
\begin{lstlisting}[language=Python]
def detect_change_points(embeddings):
    """PELT algorithm for change point detection."""
    # Project to 1D using first principal component
    pca = PCA(n_components=1)
    emb_1d = pca.fit_transform(embeddings).flatten()
    
    # PELT with adaptive penalty selection
    algo = rpt.Pelt(model='rbf', min_size=10).fit(emb_1d)
    penalty = select_penalty_elbow(emb_1d, algo)
    change_points = algo.predict(pen=penalty)
    return change_points[:-1]  # Exclude endpoint
\end{lstlisting}

\subsection{Clustering transitions}
\label{app:phase-detection-clustering-transitions}
\begin{lstlisting}[language=Python]
def detect_by_clustering(embeddings, window_size=20, step=5):
    """Detect phases via changes in clustering structure."""
    cluster_counts = []
    for i in range(0, len(embeddings) - window_size, step):
        window = embeddings[i:i+window_size]
        clustering = DBSCAN(eps=0.5, min_samples=3).fit(window)
        n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)
        cluster_counts.append(n_clusters)
    
    # Find significant changes in cluster count
    cluster_changes = np.abs(np.diff(cluster_counts))
    threshold = np.mean(cluster_changes) + 2 * np.std(cluster_changes)
    return np.where(cluster_changes > threshold)[0]
\end{lstlisting}

\section{Transport Metrics Implementation}
\label{app:transport}

For transparency and reproducibility, we provide the core implementations of our optimal transport calculations:

\subsection{Wasserstein Distance}

The 2-Wasserstein distance measures the minimum cost of transporting mass from one distribution to another:

\begin{lstlisting}[language=Python]
def compute_wasserstein_distance(traj1, traj2, metric='euclidean'):
    """
    Compute 2-Wasserstein distance between trajectories.
    Uses optimal transport to find minimum transport cost.
    """
    # Uniform weights for trajectory points
    a = np.ones(len(traj1)) / len(traj1)
    b = np.ones(len(traj2)) / len(traj2)
    
    # Cost matrix (pairwise distances)
    M = ot.dist(traj1, traj2, metric=metric)
    
    # Solve optimal transport problem
    return ot.emd2(a, b, M)
\end{lstlisting}

\subsection{Sinkhorn Divergence}

The Sinkhorn divergence provides an entropy-regularized version that is more stable in high dimensions:

\begin{lstlisting}[language=Python]
def compute_sinkhorn_divergence(traj1, traj2, regularization=0.1):
    """
    Compute debiased Sinkhorn distance.
    More robust than Wasserstein for high-dimensional data.
    """
    a = np.ones(len(traj1)) / len(traj1)
    b = np.ones(len(traj2)) / len(traj2)
    M = ot.dist(traj1, traj2)
    
    # Sinkhorn distance between trajectories
    sinkhorn_ab = ot.sinkhorn2(a, b, M, regularization)
    
    # Self-distances for debiasing
    sinkhorn_aa = ot.sinkhorn2(a, a, ot.dist(traj1, traj1), regularization)
    sinkhorn_bb = ot.sinkhorn2(b, b, ot.dist(traj2, traj2), regularization)
    
    # Debiased Sinkhorn divergence
    return sinkhorn_ab - 0.5 * (sinkhorn_aa + sinkhorn_bb)
\end{lstlisting}

\subsection{Gromov-Wasserstein Distance}

This metric compares only internal structure, making it invariant to isometric transformations:

\begin{lstlisting}[language=Python]
def compute_gromov_wasserstein(traj1, traj2):
    """
    Compute Gromov-Wasserstein distance.
    Compares internal structure without requiring same embedding space.
    """
    # Uniform distributions
    p = np.ones(len(traj1)) / len(traj1)
    q = np.ones(len(traj2)) / len(traj2)
    
    # Internal distance matrices
    C1 = ot.dist(traj1, traj1)  # Distances within traj1
    C2 = ot.dist(traj2, traj2)  # Distances within traj2
    
    # Gromov-Wasserstein distance
    return ot.gromov_wasserstein2(C1, C2, p, q)
\end{lstlisting}

All implementations use the Python Optimal Transport (POT) library with GPU acceleration when available.

\section{Statistical Agreement Metrics}
\label{app:agreement-metrics}

\subsection{Fleiss' Kappa for Multi-Model Phase Agreement}

Fleiss' kappa measures agreement among multiple raters (in our case, embedding models) for categorical assignments:

\begin{lstlisting}[language=Python]
def compute_fleiss_kappa(phase_assignments):
    """
    Compute Fleiss' kappa for phase detection agreement.
    
    Args:
        phase_assignments: Dict mapping model_name to list of phase labels
    
    Returns:
        kappa: Agreement score (-1 to 1, where 1 is perfect agreement)
    """
    from statsmodels.stats import inter_rater
    
    # Convert to matrix format: rows=messages, cols=phase categories
    n_messages = len(next(iter(phase_assignments.values())))
    n_raters = len(phase_assignments)
    
    # Create assignment matrix
    assignments = []
    for msg_idx in range(n_messages):
        msg_assignments = [phase_assignments[model][msg_idx] 
                          for model in phase_assignments]
        assignments.append(msg_assignments)
    
    # Compute kappa
    kappa = inter_rater.fleiss_kappa(assignments)
    return kappa
\end{lstlisting}

\subsection{Kendall's W for Trajectory Concordance}

Kendall's W (coefficient of concordance) measures agreement for continuous rankings or sequences:

\begin{lstlisting}[language=Python]
def compute_kendalls_w(trajectory_features):
    """
    Compute Kendall's W for trajectory feature agreement.
    
    Args:
        trajectory_features: Dict mapping model_name to feature sequence
    
    Returns:
        w: Concordance coefficient (0 to 1, where 1 is perfect agreement)
    """
    from scipy import stats
    
    # Convert to matrix: rows=models, cols=time points
    feature_matrix = np.array(list(trajectory_features.values()))
    
    # Rank each model's features
    ranked_matrix = np.apply_along_axis(stats.rankdata, 1, feature_matrix)
    
    # Compute W
    n_models, n_points = ranked_matrix.shape
    mean_rank = np.mean(ranked_matrix, axis=0)
    
    # Sum of squared deviations from mean rank
    S = np.sum((np.sum(ranked_matrix, axis=0) - n_models * mean_rank) ** 2)
    
    # Maximum possible S
    max_S = (n_models ** 2) * (n_points ** 3 - n_points) / 12
    
    w = S / max_S if max_S > 0 else 0
    return w
\end{lstlisting}

\section{Bootstrap Validation Methods}
\label{app:bootstrap}

\subsection{Parametric Bootstrap for Correlation Confidence Intervals}

We use parametric bootstrap to construct confidence intervals and test significance of invariance scores:

\begin{lstlisting}[language=Python]
def bootstrap_invariance_confidence(invariance_scores, n_bootstrap=10000):
    """
    Compute bootstrap confidence intervals for invariance scores.
    
    Uses parametric bootstrap assuming normal distribution of correlations
    after Fisher z-transformation.
    """
    # Extract statistics by signature type
    signature_stats = invariance_scores['signature_type_stats']
    bootstrap_means = []
    
    for _ in range(n_bootstrap):
        # Sample from each signature type
        bootstrap_sample = []
        for sig_type, stats in signature_stats.items():
            # Fisher z-transform for correlation
            z_mean = np.arctanh(stats['mean'])
            z_std = stats['std'] / (1 - stats['mean']**2)
            
            # Sample in z-space
            z_samples = np.random.normal(z_mean, z_std, 
                                       stats['n_conversations'])
            
            # Transform back to correlation space
            r_samples = np.tanh(z_samples)
            bootstrap_sample.extend(r_samples)
        
        bootstrap_means.append(np.mean(bootstrap_sample))
    
    # Compute confidence intervals
    ci_lower = np.percentile(bootstrap_means, 2.5)
    ci_upper = np.percentile(bootstrap_means, 97.5)
    
    return {
        'mean': np.mean(bootstrap_means),
        'ci_95': (ci_lower, ci_upper),
        'se': np.std(bootstrap_means)
    }
\end{lstlisting}

\subsection{Correlation Hierarchy Testing}
\label{app:correlation-hierarchy-testing}

Test whether within-paradigm correlations exceed cross-paradigm correlations:

\begin{lstlisting}[language=Python]
def test_correlation_hierarchy(within_transformer, within_classical, cross_paradigm):
    """
    Test if correlation hierarchy holds: within > cross > chance.
    
    Uses Mann-Whitney U test for non-parametric comparison.
    """
    from scipy import stats
    
    # Flatten correlation lists
    within_all = np.concatenate([within_transformer, within_classical])
    
    # Test within > cross
    u_stat, p_within_cross = stats.mannwhitneyu(
        within_all, cross_paradigm, alternative='greater'
    )
    
    # Test cross > chance (0.5)
    t_stat, p_cross_chance = stats.ttest_1samp(
        cross_paradigm, popmean=0.0, alternative='greater'
    )
    
    # Effect sizes (rank-biserial correlation)
    n1, n2 = len(within_all), len(cross_paradigm)
    r_effect = 1 - (2 * u_stat) / (n1 * n2)
    
    return {
        'within_vs_cross_p': p_within_cross,
        'cross_vs_chance_p': p_cross_chance,
        'effect_size': r_effect,
        'hierarchy_confirmed': p_within_cross < 0.05 and p_cross_chance < 0.05
    }
\end{lstlisting}

\section{Null Model Implementations}
\label{app:null-models}

To ensure our invariance findings reflect genuine conversational structure rather than statistical artifacts, we implement multiple null models that disrupt different aspects of the data:

\subsection{Phase Scrambling}

Phase scrambling preserves the power spectrum while destroying temporal coherence:

\begin{lstlisting}[language=Python]
def phase_scramble_embeddings(embeddings):
    """
    Preserve frequency content but destroy temporal structure.
    Ensures proper conjugate symmetry for real-valued signals.
    """
    n_messages, embedding_dim = embeddings.shape
    scrambled = np.zeros_like(embeddings)
    
    for dim in range(embedding_dim):
        signal_1d = embeddings[:, dim]
        
        # Apply FFT
        fft = np.fft.fft(signal_1d)
        magnitudes = np.abs(fft)
        phases = np.angle(fft)
        
        # Generate random phases (-pi to pi)
        random_phases = np.random.uniform(-np.pi, np.pi, size=len(phases))
        random_phases[0] = phases[0]  # Preserve DC component
        
        # Ensure conjugate symmetry for real signals
        if n_messages % 2 == 0:
            # Even length - special handling for Nyquist frequency
            random_phases[n_messages//2] = 0
            random_phases[n_messages//2+1:] = -random_phases[1:n_messages//2][::-1]
        else:
            # Odd length
            random_phases[n_messages//2+1:] = -random_phases[1:n_messages//2+1][::-1]
        
        # Reconstruct with randomized phases
        fft_scrambled = magnitudes * np.exp(1j * random_phases)
        scrambled[:, dim] = np.real(np.fft.ifft(fft_scrambled))
    
    return scrambled
\end{lstlisting}

\subsection{Message-Level Scrambling}

Destroys sequential structure while preserving individual message content:

\begin{lstlisting}[language=Python]
def generate_message_shuffle_null(conversation):
    """Shuffle message order while preserving individual messages."""
    null_conv = copy.deepcopy(conversation)
    random.shuffle(null_conv['messages'])
    return null_conv

def shuffle_within_speakers(messages, embeddings):
    """Shuffle messages within each speaker, preserving speaker patterns."""
    # Group by speaker
    speaker_indices = {}
    for i, msg in enumerate(messages):
        role = msg.get('role', 'unknown')
        if role not in speaker_indices:
            speaker_indices[role] = []
        speaker_indices[role].append(i)
    
    # Shuffle indices within each speaker group
    new_order = []
    for role, indices in speaker_indices.items():
        shuffled = indices.copy()
        random.shuffle(shuffled)
        new_order.extend(shuffled)
    
    return embeddings[new_order]
\end{lstlisting}

\subsection{Random Walk Nulls}

Generate trajectories with matched statistics but no semantic coherence:

\begin{lstlisting}[language=Python]
def generate_random_walk(conversation):
    """Generate random walk with matched vocabulary and length statistics."""
    null_conv = copy.deepcopy(conversation)
    
    # Extract vocabulary and message length statistics
    message_lengths = [len(msg['content'].split()) for msg in conversation['messages']]
    vocab = set()
    for msg in conversation['messages']:
        vocab.update(msg['content'].lower().split())
    vocab_list = list(vocab)
    
    # Generate random messages with similar lengths
    for i, msg in enumerate(null_conv['messages']):
        target_length = message_lengths[i]
        words = np.random.choice(vocab_list, size=target_length, replace=True)
        msg['content'] = ' '.join(words)
    
    return null_conv
\end{lstlisting}

\subsection{Paradigm-Specific Controls}

\subsubsection{Classical Embedding Nulls (Averaging)}

For Word2Vec and GloVe, we simulate loss of positional information:

\begin{lstlisting}[language=Python]
def generate_averaging_null(embeddings, window_sizes=[1, 3, 5, 10]):
    """
    Generate null model for averaging-based embeddings.
    Accounts for the fact that classical models average word embeddings.
    """
    nulls = {}
    shuffled = embeddings.copy()
    np.random.shuffle(shuffled)
    
    for window in window_sizes:
        if window == 1:
            nulls[f'avg_window_{window}'] = shuffled
        else:
            # Apply moving average
            averaged = np.zeros_like(shuffled)
            for i in range(len(shuffled)):
                start = max(0, i - window // 2)
                end = min(len(shuffled), i + window // 2 + 1)
                averaged[i] = np.mean(shuffled[start:end], axis=0)
            nulls[f'avg_window_{window}'] = averaged
    
    return nulls
\end{lstlisting}

\subsubsection{Transformer Nulls (Positional Encoding)}

For transformer models, we disrupt positional information:

\begin{lstlisting}[language=Python]
def generate_positional_null(embeddings, max_position=512):
    """
    Generate null model with randomized positional encodings.
    Accounts for transformers' use of position information.
    """
    n_messages, embedding_dim = embeddings.shape
    
    # Generate random positions
    positions = np.random.randint(0, min(max_position, n_messages), size=n_messages)
    
    # Create sinusoidal position encodings
    position_encodings = np.zeros((n_messages, embedding_dim))
    
    for pos in range(n_messages):
        for i in range(0, embedding_dim, 2):
            position_encodings[pos, i] = np.sin(
                positions[pos] / (10000 ** (i / embedding_dim))
            )
            if i + 1 < embedding_dim:
                position_encodings[pos, i + 1] = np.cos(
                    positions[pos] / (10000 ** (i / embedding_dim))
                )
    
    # Shuffle embeddings and add position encodings
    shuffled = embeddings.copy()
    np.random.shuffle(shuffled)
    
    # Scale position encodings to 10% of embedding magnitude
    scale = np.std(embeddings) * 0.1
    
    return shuffled + scale * position_encodings
\end{lstlisting}

\subsection{Structured Null Models}

Additional null models that preserve specific conversation properties:

\begin{lstlisting}[language=Python]
def generate_topic_preserving_null(embeddings, n_topics=5):
    """
    Preserve topic structure but destroy trajectory.
    Tests whether invariance is due to topic consistency alone.
    """
    # Identify topics using clustering
    kmeans = KMeans(n_clusters=n_topics, random_state=42)
    topic_labels = kmeans.fit_predict(embeddings)
    topic_centers = kmeans.cluster_centers_
    
    null_embeddings = []
    for topic_idx in topic_labels:
        # Sample from topic distribution
        center = topic_centers[topic_idx]
        topic_points = embeddings[topic_labels == topic_idx]
        
        if len(topic_points) > 1:
            cov = np.cov(topic_points.T) + 1e-4 * np.eye(len(center))
            new_point = multivariate_normal.rvs(mean=center, cov=cov)
        else:
            new_point = center + np.random.randn(len(center)) * 0.1
        
        null_embeddings.append(new_point)
    
    return np.array(null_embeddings)

def generate_conversation_structure_null(embeddings, preserve_local=True, local_window=5):
    """
    Preserve local structure but destroy global patterns.
    Tests importance of long-range dependencies.
    """
    if not preserve_local:
        null = embeddings.copy()
        np.random.shuffle(null)
        return null
    
    n_messages = len(embeddings)
    n_chunks = n_messages // local_window
    
    # Divide into chunks
    chunks = []
    for i in range(n_chunks):
        start = i * local_window
        end = min((i + 1) * local_window, n_messages)
        chunks.append(embeddings[start:end].copy())
    
    # Shuffle chunks
    np.random.shuffle(chunks)
    
    # Handle remainder
    remainder = n_messages % local_window
    if remainder > 0:
        chunks.append(embeddings[-remainder:])
    
    return np.vstack(chunks)[:n_messages]
\end{lstlisting}

\subsection{Null Model Validation}

We validate that null models preserve intended properties:

\begin{lstlisting}[language=Python]
def test_null_validity(original, null):
    """
    Verify null model preserves desired properties while destroying structure.
    """
    metrics = {}
    
    # Check mean preservation
    metrics['mean_difference'] = np.abs(np.mean(original) - np.mean(null))
    
    # Check variance preservation
    metrics['variance_ratio'] = np.var(null) / np.var(original)
    
    # Check temporal autocorrelation (should be destroyed)
    def autocorr(x, lag=1):
        n = len(x)
        c0 = np.dot(x[:-lag], x[:-lag])
        c1 = np.dot(x[:-lag], x[lag:])
        return c1 / c0 if c0 > 0 else 0
    
    orig_autocorr = np.mean([autocorr(original[:, i]) for i in range(original.shape[1])])
    null_autocorr = np.mean([autocorr(null[:, i]) for i in range(null.shape[1])])
    
    metrics['autocorr_reduction'] = (orig_autocorr - null_autocorr) / orig_autocorr
    
    # For phase scrambling: verify power spectrum preservation
    orig_power = np.mean([np.abs(np.fft.fft(original[:, i]))**2 
                         for i in range(original.shape[1])])
    null_power = np.mean([np.abs(np.fft.fft(null[:, i]))**2 
                         for i in range(null.shape[1])])
    
    metrics['power_ratio'] = null_power / orig_power
    
    return metrics
\end{lstlisting}

\section{Control Analysis Implementations}
\label{app:control-analyses}

\subsection{Message Length Control}

We control for potential confounds from message length variations:

\begin{lstlisting}[language=Python]
def control_for_message_length(embeddings, message_lengths, correlation_func):
    """
    Control for message length effects on correlations.
    Uses regression to remove length-related variance.
    """
    results = {}
    
    # Compute raw correlation
    raw_corr = correlation_func(embeddings)
    results['raw_correlation'] = raw_corr
    
    # Check if length correlates with embedding distances
    trajectory_distances = np.linalg.norm(np.diff(embeddings, axis=0), axis=1)
    length_diffs = np.diff(message_lengths)
    
    if np.std(length_diffs) > 0:
        length_corr, length_p = stats.pearsonr(trajectory_distances, length_diffs[:-1])
    else:
        length_corr, length_p = 0.0, 1.0
    
    # Regress out length effects from each embedding dimension
    embeddings_controlled = embeddings.copy()
    for dim in range(embeddings.shape[1]):
        reg = LinearRegression()
        reg.fit(message_lengths.reshape(-1, 1), embeddings[:, dim])
        residuals = embeddings[:, dim] - reg.predict(message_lengths.reshape(-1, 1))
        embeddings_controlled[:, dim] = residuals
    
    # Compute correlation on length-controlled embeddings
    controlled_corr = correlation_func(embeddings_controlled)
    
    results['partial_correlation'] = controlled_corr
    results['correlation_change'] = abs(raw_corr - controlled_corr)
    results['relative_change'] = results['correlation_change'] / abs(raw_corr)
    
    return results
\end{lstlisting}

\subsection{Dimension Normalization}

Account for different embedding dimensionalities across models:

\begin{lstlisting}[language=Python]
def normalize_dimensions(embeddings_dict, target_dim=300):
    """
    Project all embeddings to common dimensionality.
    Uses PCA to preserve maximum variance.
    """
    normalized = {}
    
    for model_name, embeddings in embeddings_dict.items():
        n_messages, source_dim = embeddings.shape
        
        if source_dim == target_dim:
            normalized[model_name] = embeddings
        elif source_dim > target_dim:
            # Project down using PCA
            pca = PCA(n_components=target_dim)
            normalized[model_name] = pca.fit_transform(embeddings)
        else:
            # Pad with zeros (preserves original structure)
            padded = np.zeros((n_messages, target_dim))
            padded[:, :source_dim] = embeddings
            normalized[model_name] = padded
    
    return normalized
\end{lstlisting}

\subsection{Outlier Robustness Check}

Ensure results aren't driven by outlier conversations:

\begin{lstlisting}[language=Python]
def outlier_robustness_check(all_correlations, outlier_threshold=3.0):
    """
    Check robustness to outlier conversations.
    Uses z-score method to identify outliers.
    """
    results = {}
    
    # Identify outliers
    z_scores = np.abs(stats.zscore(all_correlations))
    outlier_mask = z_scores > outlier_threshold
    
    n_outliers = np.sum(outlier_mask)
    results['n_outliers'] = n_outliers
    results['outlier_percentage'] = 100 * n_outliers / len(all_correlations)
    
    # Compare statistics with and without outliers
    results['with_outliers'] = {
        'mean': np.mean(all_correlations),
        'median': np.median(all_correlations),
        'std': np.std(all_correlations)
    }
    
    clean_correlations = all_correlations[~outlier_mask]
    results['without_outliers'] = {
        'mean': np.mean(clean_correlations),
        'median': np.median(clean_correlations),
        'std': np.std(clean_correlations)
    }
    
    # Test if conclusions change
    hypothesis_threshold = 0.5
    results['conclusion_robust'] = (
        (results['with_outliers']['mean'] > hypothesis_threshold) ==
        (results['without_outliers']['mean'] > hypothesis_threshold)
    )
    
    return results
\end{lstlisting}

\subsection{Temporal Stability Analysis}

Check for drift in patterns over data collection period:

\begin{lstlisting}[language=Python]
def temporal_stability_analysis(conversations, window_size=50):
    """
    Analyze temporal stability using rolling windows.
    Tests for stationarity using Augmented Dickey-Fuller.
    """
    # Sort by timestamp
    conversations = sorted(conversations, key=lambda x: x['timestamp'])
    
    # Compute rolling window correlations
    rolling_correlations = []
    n_windows = len(conversations) - window_size + 1
    
    for i in range(n_windows):
        window_convs = conversations[i:i+window_size]
        window_corr = compute_window_correlation(window_convs)
        rolling_correlations.append(window_corr)
    
    # Test for stationarity
    from statsmodels.tsa.stattools import adfuller
    adf_result = adfuller(rolling_correlations)
    
    # Trend analysis
    x = np.arange(len(rolling_correlations))
    slope, intercept, r_value, p_value, std_err = stats.linregress(
        x, rolling_correlations
    )
    
    return {
        'is_stationary': adf_result[1] < 0.05,
        'adf_p_value': adf_result[1],
        'trend_slope': slope,
        'trend_p_value': p_value,
        'trend_r_squared': r_value**2
    }
\end{lstlisting}

\subsection{Cross-Validation}

Ensure findings generalize across conversation subsets:

\begin{lstlisting}[language=Python]
def cross_validation_analysis(conversations, n_folds=5):
    """
    K-fold cross-validation of invariance findings.
    Tests stability across different conversation subsets.
    """
    from sklearn.model_selection import KFold
    
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    fold_results = []
    
    for fold, (train_idx, test_idx) in enumerate(kf.split(conversations)):
        train_convs = [conversations[i] for i in train_idx]
        test_convs = [conversations[i] for i in test_idx]
        
        # Compute invariance on training set
        train_invariance = compute_invariance_scores(train_convs)
        
        # Validate on test set
        test_invariance = compute_invariance_scores(test_convs)
        
        fold_results.append({
            'fold': fold,
            'train_score': train_invariance['mean'],
            'test_score': test_invariance['mean'],
            'generalization_gap': abs(train_invariance['mean'] - 
                                    test_invariance['mean'])
        })
    
    # Aggregate results
    test_scores = [r['test_score'] for r in fold_results]
    gaps = [r['generalization_gap'] for r in fold_results]
    
    return {
        'mean_test_score': np.mean(test_scores),
        'std_test_score': np.std(test_scores),
        'mean_generalization_gap': np.mean(gaps),
        'stable_across_folds': np.std(test_scores) < 0.1
    }
\end{lstlisting}

\subsection{Bootstrap Confidence Intervals}

Compute robust confidence intervals for all statistics:

\begin{lstlisting}[language=Python]
def bootstrap_confidence_intervals(data, statistic_func, 
                                 n_bootstrap=1000, confidence=0.95):
    """
    Non-parametric bootstrap for confidence intervals.
    Works for any statistic function.
    """
    original_stat = statistic_func(data)
    bootstrap_stats = []
    n_samples = len(data)
    
    for _ in range(n_bootstrap):
        # Resample with replacement
        bootstrap_sample = np.random.choice(data, size=n_samples, replace=True)
        bootstrap_stats.append(statistic_func(bootstrap_sample))
    
    # Compute confidence intervals
    alpha = 1 - confidence
    lower = np.percentile(bootstrap_stats, 100 * (alpha / 2))
    upper = np.percentile(bootstrap_stats, 100 * (1 - alpha / 2))
    
    return {
        'statistic': original_stat,
        'ci_lower': lower,
        'ci_upper': upper,
        'bootstrap_se': np.std(bootstrap_stats),
        'bias': np.mean(bootstrap_stats) - original_stat
    }
\end{lstlisting}



\section{Selected Conversation Analysis Results}

To illustrate the global-local dichotomy across model capability tiers, we present detailed ensemble analysis results for six representative conversations. Each figure shows the complete geometric analysis pipeline output: distance matrices, self-similarity patterns, recurrence plots, embedding trajectories, density evolution, and phase detection results across all five embedding models.

\afterpage{%
\clearpage
\thispagestyle{empty}
\includepdf[pages=1, pagecommand={\thispagestyle{plain}\label{fig:full_reasoning_1}}]{../analysis/analysis_output/figures/ensemble/pdf/Consciousness_Exploration_2025-06-16_19-Y_ensemble.pdf}
}

\afterpage{%
\clearpage
\thispagestyle{empty}
\includepdf[pages=1, pagecommand={\thispagestyle{plain}\label{fig:full_reasoning_2}}]{../analysis/analysis_output/figures/ensemble/pdf/experiment-consciousness-exploration-session-consciousness_exploration-2025-07-29-6-8634a018_ensemble.pdf}
}

\afterpage{%
\clearpage
\thispagestyle{empty}
\includepdf[pages=1, pagecommand={\thispagestyle{plain}\label{fig:light_reasoning_1}}]{../analysis/analysis_output/figures/ensemble/pdf/experiment-consciousness-exploration-efficient-models-session-consciousness_exploration_efficient_models-2025-07-22-9-f5a3afc3_ensemble.pdf}
}

\afterpage{%
\clearpage
\thispagestyle{empty}
\includepdf[pages=1, pagecommand={\thispagestyle{plain}\label{fig:light_reasoning_2}}]{../analysis/analysis_output/figures/ensemble/pdf/experiment-consciousness-exploration-efficient-models-session-consciousness_exploration_efficient_models-2025-07-28-13-d910cc18_ensemble.pdf}
}

\afterpage{%
\clearpage
\thispagestyle{empty}
\includepdf[pages=1, pagecommand={\thispagestyle{plain}\label{fig:non_reasoning_1}}]{../analysis/analysis_output/figures/ensemble/pdf/experiment-non-reasoning-session-non_reasoning-2025-07-28-1-cdbe1349_ensemble.pdf}
}

\afterpage{%
\clearpage
\thispagestyle{empty}
\includepdf[pages=1, pagecommand={\thispagestyle{plain}\label{fig:non_reasoning_2}}]{../analysis/analysis_output/figures/ensemble/pdf/experiment-non-reasoning-session-non_reasoning-2025-07-25-7-280fbd98_ensemble.pdf}
}

\end{document}