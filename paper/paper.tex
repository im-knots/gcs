\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{array}
\usepackage{longtable}
\usepackage{xfp}
\usepackage{siunitx}
\usepackage{newunicodechar}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{tcolorbox}
\newunicodechar{âˆž}{\ensuremath{\infty}}

% Data placeholders - UPDATED with actual values from analysis
\newcommand{\totalConversations}{229}
\newcommand{\numEmbeddingModels}{5}

% Core findings from actual data
\newcommand{\meanInvarianceScore}{0.377}
\newcommand{\medianInvarianceScore}{0.547}

% Distance matrix correlations from heatmaps
\newcommand{\distanceMatrixCorrelation}{0.879}
\newcommand{\distanceCorrRange}{[0.521, 0.957]}
\newcommand{\minPairwiseCorr}{0.521}
\newcommand{\maxPairwiseCorr}{0.957}

% Model-specific correlations from data
\newcommand{\transformerInternalCorr}{0.827}
\newcommand{\classicalInternalCorr}{0.934}
\newcommand{\crossParadigmCorr}{0.616}

% Specific model pair correlations from heatmap
\newcommand{\miniLMmpnetCorr}{0.855}
\newcommand{\miniLMwordTovecCorr}{0.659}
\newcommand{\wordTovecGloveCorr}{0.931}

% Visual pattern metrics
\newcommand{\blockPatternConsistency}{high}
\newcommand{\trajectoryShapeAgreement}{moderate}
\newcommand{\densityPeakAlignment}{variable}

% Phase detection performance - HIGHLY VARIABLE
\newcommand{\phaseDetectionF}{0.16}
\newcommand{\phaseAgreementRange}{[-0.14, 0.76]}
\newcommand{\phaseDetectionBest}{0.76}
\newcommand{\phaseDetectionWorst}{-0.14}
\newcommand{\phaseDetectionFRange}{[0.08, 0.36]}

% Statistical validation
\newcommand{\bootstrapIterations}{10000}
\newcommand{\nullModelPValue}{0.001}
\newcommand{\nullBaselineCorr}{0.082}

% Hypothesis test results
\newcommand{\tiersPassed}{3/3}
\newcommand{\meanEffectSize}{0.152}
\newcommand{\hOnePvalue}{2.8606e-08}
\newcommand{\hTwoPvalue}{1.4507e-10}

% Conversation metrics
\newcommand{\minConvLength}{118}
\newcommand{\maxConvLength}{235}
\newcommand{\meanConvLength}{183.2}

% Figure paths
\newcommand{\figuresPath}{./figures/}

\title{Geometric Signatures in Conversation: Evidence for Structural Invariance Despite Local Variability}

\author{
Marco R. Garcia \\
marco@erulabs.ai
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present empirical evidence that conversations exhibit consistent geometric signatures when projected into different embedding spaces, alongside surprising variability in local feature detection. Analyzing \totalConversations{} multi-agent AI dialogues from our prior study on social dynamics \citep{garcia2025peer}, we examine whether geometric properties of conversational trajectories remain consistent across \numEmbeddingModels{} fundamentally different embedding models. Our analysis reveals a paradox: while global geometric patterns (distance matrices, trajectory shapes) show remarkable consistency across both transformer-based and classical embeddings (correlations ranging from \minPairwiseCorr{} to \maxPairwiseCorr{}), local phase detection exhibits extreme variability (F1 scores from 0.08 to 0.36, agreement correlations from -0.14 to 0.76). This dichotomy (high global consistency, low local agreement) suggests that conversations possess robust geometric structure at macroscopic scales while microscopic features remain model-dependent. These findings establish that geometric analysis of conversation captures genuine structural properties rather than model artifacts, but highlight important limitations for fine-grained applications.
\end{abstract}

\section{Introduction}

In our recent investigation of AI social dynamics \citep{garcia2025peer}, we documented unexpected peer pressure effects in multi-agent conversations. These behavioral phenomena (breakdown cascades, recovery mechanisms, behavioral territories) revealed complex dynamics that traditional analysis methods struggled to explain. This observation motivated an exploratory question: might conversations have underlying mathematical structure that could provide new perspectives on these dynamics?

The present work investigates a specific empirical question: when we project conversations into embedding spaces using different models, do we observe consistent geometric patterns? This is not an attempt to build a comprehensive theory, but rather an exploration of whether mathematical tools might offer useful lenses for understanding conversation.

We test for geometric invariance using \totalConversations{} multi-agent dialogues from our social dynamics study. By examining conversational trajectories through \numEmbeddingModels{} diverse embedding models (including both transformer and classical approaches), we can assess whether observed patterns reflect genuine conversational structure or merely artifacts of specific architectures.

Our key empirical findings reveal an unexpected dichotomy:
\begin{enumerate}
\item \textbf{Global geometry is remarkably consistent}: Distance matrices and trajectory shapes show strong correlations across all embedding models
\item \textbf{Local features are surprisingly variable}: Phase detection and fine-grained structure show poor cross-model agreement
\item \textbf{This pattern persists across paradigms}: Even fundamentally different approaches agree on macro-structure while disagreeing on micro-structure
\end{enumerate}

\section{Background and Related Work}

\subsection{The Geometric Revolution in NLP}

Recent years have witnessed a fundamental shift in understanding language through mathematical lenses. \citet{reif2019visualizing} pioneered geometric analysis of transformer embeddings, revealing that BERT naturally segregates semantic and syntactic information into distinct subspaces. This work established that language models encode surprisingly rich geometric structure beyond simple vector similarities.

The geometry of embedding spaces has proven more complex than initially assumed. \citet{ethayarajh2019contextual} demonstrated that contextual embeddings exhibit extreme anisotropy, clustering within narrow cones occupying less than 1\% of available vector space. Paradoxically, this apparent inefficiency correlates with improved performance, challenging assumptions about optimal embedding geometry. Recent work on intrinsic dimensionality \citep{aghajanyan2021intrinsic} further shows that language models compress task representations into remarkably low-dimensional subspaces (200-dimensional projections achieving 90\% of full fine-tuning performance).

\subsection{Conversational Trajectories as Mathematical Objects}

While geometric analysis of static embeddings has advanced rapidly, applications to conversational dynamics remain nascent. \citet{brinberg2024dynamic} introduced trajectory-based analysis modeling dialogues as paths through behavioral state spaces. This framework visualizes how dyads navigate conversational territories, revealing attractor states and phase transitions.

Recent work explicitly treating conversations as trajectories includes \citet{toxicity2025trajectory} who model user posting histories as continuous paths through topic space, and \citet{psychiatric2024semantic} who quantify semantic trajectories in clinical speech. Clinical applications demonstrate the power of trajectory analysis. \citet{palominos2024trajectories} showed that conversations involving individuals with mental health conditions exhibit measurably different geometric properties (shrinking semantic spaces, reduced convex hull volumes), providing quantitative biomarkers for mental health assessment.

The mathematical formalization of conversational dynamics has progressed through several frameworks. \citet{clarfeld2020codym} introduce CODYMs (COnversational DYnamics Model) using Markov models to capture sequential dependencies in palliative care conversations. \citet{fischer2024personality} map 1,655 conversations into 768-dimensional spaces, showing how personality differences affect geometric distances in conversation space.

\subsection{Cross-Model Embedding Analysis}

The question of how different embedding models relate has received attention primarily through transfer learning and alignment. Recent work on the "Platonic Representation Hypothesis" \citep{huh2024platonic} suggests diverse neural networks converge to similar representations, but this has been tested primarily on static embeddings rather than conversational trajectories.

\citet{dialoguecse2021} demonstrate context-response semantic matching through contrastive learning, showing how these approaches maintain better consistency across dialogue contexts than siamese networks. \citet{dial2vec2022} compare pre-trained language models with dialogue-specific approaches, revealing 8.7-13.8 point improvements by capturing speaker interaction patterns.

Studies on embedding space alignment typically focus on finding transformations between spaces \citep{conneau2018word} for practical applications. The question of whether conversational structures exhibit invariant geometric properties across models appears unexplored in the literature.

\subsection{Topological and Geometric Methods}

Topological data analysis (TDA) has emerged as powerful for understanding high-dimensional language data. \citet{zhu2013persistent} introduced persistent homology for text analysis, with recent applications specifically to dialogue. \citet{vukovic2022dialogue} apply persistent homology to dialogue systems, using topological features of word embedding spaces that outperform pure embedding methods on MultiWOZ datasets. \citet{ruppik2024topology} introduce complexity measures for local topology in contextual language model latent spaces, exploring the manifold hypothesis for dialogue contexts.

Particularly relevant is \citet{jakubowski2020topology}, who argue that word embeddings live on pinched manifolds where singular points correspond to polysemous words (directly applicable to understanding conversational ambiguity). These topological approaches reveal structure invisible to traditional analysis methods.

The geometric deep learning paradigm \citep{bronstein2021geometric} provides theoretical frameworks for understanding neural architectures through symmetries and invariances. While this perspective has revolutionized graph neural networks for dialogue \citep{ghosal2019dialoguegcn}, the specific question of geometric invariance in conversational embeddings remains unaddressed.

\subsection{Positioning Our Contribution}

Our work sits at an unexplored intersection of these research streams:

\begin{itemize}
\item Prior work analyzed embedding geometry for static representations, not dynamic trajectories
\item Conversational trajectory studies use single models, not cross-model validation
\item Embedding alignment literature focuses on transformation finding, not invariant properties
\item Topological methods concentrate on documents or single-model dialogue analysis, not cross-model conversational evolution
\end{itemize}

We ask a fundamentally different question: do conversations exhibit geometric signatures that remain consistent across different embedding models? This invariance question, if answered affirmatively, would suggest geometric properties of conversation transcend specific architectures (a necessary precondition for geometric analysis to be meaningful). Our exploratory approach differs from hypothesis-driven research by investigating whether mathematical tools might provide useful perspectives on the complex dynamics observed in our prior work \citep{garcia2025peer}.

\section{Methodology}

\subsection{Data}

We analyze \totalConversations{} conversations from our prior study on AI social dynamics \citep{garcia2025peer}. These multi-agent dialogues between AI models discussing consciousness were originally collected to study peer pressure and breakdown patterns. The corpus includes:

\begin{itemize}
\item Full reasoning models (N=67): Claude 3 Opus, GPT-4, Gemini Ultra
\item Light reasoning models (N=61): Claude 3.5 Sonnet, GPT-4 Mini, Gemini Pro
\item Non-reasoning models (N=100): Claude 3.5 Haiku, GPT-4 Turbo, Gemini Flash
\end{itemize}

Conversations range from \minConvLength{} to \maxConvLength{} messages (mean = \meanConvLength{}), with rich dynamics including topic evolution, phase transitions, and the peer pressure effects that motivated this geometric investigation.

\subsection{Embedding Models}

To test invariance across paradigms, we employ an ensemble approach:

\textbf{Transformer-based (contextual embeddings):}
\begin{itemize}
\item all-MiniLM-L6-v2: 6-layer, 384-dimensional sentence embeddings
\item all-mpnet-base-v2: 12-layer, 768-dimensional embeddings optimized for semantic similarity
\item all-MiniLM-L12-v2: 12-layer, 384-dimensional model balancing quality and efficiency
\end{itemize}

\textbf{Classical (static embeddings):}
\begin{itemize}
\item Word2Vec: 300-dimensional skip-gram model trained on Google News corpus
\item GloVe: 300-dimensional global vectors trained on Common Crawl (840B tokens)
\end{itemize}

For classical models, sentence embeddings are computed by averaging word vectors after removing stop words. All embedding generation uses GPU acceleration for batch processing.

\subsection{Geometric Analysis Pipeline}

\subsubsection{Geometric Signature Computation}

For each conversation and embedding model, we compute:
\begin{itemize}
\item \textbf{Distance matrices}: Pairwise Euclidean distances between all message embeddings
\item \textbf{Trajectory metrics}: Velocity, acceleration, and curvature at each conversation turn
\item \textbf{Density evolution}: Local density changes using sliding window kernel density estimation
\item \textbf{Topological features}: Persistence diagrams and Betti numbers
\end{itemize}

The pipeline includes three trajectory normalization methods:
\begin{itemize}
\item \textbf{Adaptive normalization}: Scales metrics based on conversation dynamics
\item \textbf{Entropy-based normalization}: Accounts for information content
\item \textbf{Information-theoretic normalization}: Uses mutual information between consecutive messages
\end{itemize}

\subsubsection{Phase Detection Methods}

We implement ensemble phase detection using four complementary approaches:
\begin{itemize}
\item \textbf{Embedding shift detection}: Identifies changes in centroid movement with adaptive thresholding using Median Absolute Deviation (MAD)
\item \textbf{Change point detection}: Applies PELT (Pruned Exact Linear Time) and binary segmentation algorithms
\item \textbf{Clustering transitions}: Uses DBSCAN to identify shifts in local clustering patterns
\item \textbf{Velocity-based detection}: Finds peaks in trajectory velocity exceeding dynamic thresholds
\end{itemize}

Each method votes on potential phase boundaries, with consensus determined by weighted voting based on method-specific confidence scores.

\subsubsection{Invariance Analysis}

Cross-model invariance is assessed through:
\begin{itemize}
\item \textbf{Pairwise correlations}: Spearman correlations between distance matrices, trajectory metrics, and density patterns
\item \textbf{Model agreement statistics}: Fleiss' kappa and Kendall's W for phase detection consensus
\item \textbf{Paradigm-specific comparisons}: Within-transformer, within-classical, and cross-paradigm correlations
\end{itemize}

\subsection{Statistical Validation}

We employ a hierarchical hypothesis testing framework with multiple comparison corrections:

\subsubsection{Hierarchical Testing Structure}

\begin{enumerate}
\item \textbf{Tier 1 - Within-paradigm invariance}: Tests whether models within the same paradigm show strong agreement
   \begin{itemize}
   \item H1a: Transformer models show strong correlations ($\rho > 0.75$)
   \item H1b: Classical models show strong correlations ($\rho > 0.70$)
   \item H1c: Within-paradigm correlations exceed chance (Mann-Whitney U test)
   \item Bonferroni correction applied ($\alpha = 0.017$)
   \end{itemize}

\item \textbf{Tier 2 - Cross-paradigm invariance}: Tests whether different paradigms show substantial agreement
   \begin{itemize}
   \item H2a: Cross-paradigm correlations are substantial ($\rho > 0.50$)
   \item H2b: All cross-paradigm correlations are positive (binomial test)
   \item H2c: Cross-paradigm exceeds random embeddings (Mann-Whitney U test)
   \item False Discovery Rate (FDR) correction using Benjamini-Hochberg method ($\alpha = 0.05$)
   \end{itemize}

\item \textbf{Tier 3 - Invariance hierarchy}: Tests whether within > cross > random ordering holds
   \begin{itemize}
   \item H3a: Ordering confirmed via Kruskal-Wallis test
   \item H3b: Effect size between levels is meaningful (Cohen's q > 0.3)
   \item H3c: Hierarchy persists across geometric metrics (> 80\% consistency)
   \item No correction applied (effect size criteria)
   \end{itemize}
\end{enumerate}

Each tier is only tested if the previous tier passes, controlling family-wise error rate. All correlation tests use Fisher's z-transformation for proper statistical inference. Effect sizes are calculated using Cohen's q for correlation differences and rank-biserial correlation for non-parametric tests.

\subsubsection{Null Models}

We generate paradigm-specific null models:
\begin{itemize}
\item \textbf{Phase scrambling}: Preserves power spectrum while destroying temporal structure
\item \textbf{Averaging nulls}: For classical embeddings, averages consecutive embeddings
\item \textbf{Positional nulls}: For transformers, applies random positional encodings
\item \textbf{Message-level scrambling}: Shuffles messages while preserving length distribution
\end{itemize}

\subsubsection{Control Analyses}

To ensure robustness, we implement three control hypotheses tested independently with FDR correction:

\begin{itemize}
\item \textbf{H4 - Real vs. scrambled}: Tests whether real conversations show higher correlations than message-order scrambled versions (Mann-Whitney U test)
\item \textbf{H5 - Message length control}: Partial correlations controlling for conversation length, tested via t-distribution with $n-2$ degrees of freedom
\item \textbf{H6 - Dimension normalization}: Tests whether patterns persist after normalizing for embedding dimensionality differences
\end{itemize}

Additional robustness checks include:
\begin{itemize}
\item \textbf{Bootstrap confidence intervals}: \bootstrapIterations{} iterations with stratified sampling
\item \textbf{Outlier robustness}: Winsorization at 5th and 95th percentiles
\item \textbf{Temporal stability}: Split-half reliability testing
\item \textbf{Power analysis}: Statistical power calculated for all parametric tests using effect size and sample size
\end{itemize}

\subsection{Implementation Details}

The analysis pipeline is implemented in Python 3.8+ with GPU acceleration via PyTorch. Key specifications:
\begin{itemize}
\item \textbf{Batch processing}: Conversations processed in batches of 25 for memory efficiency
\item \textbf{Checkpointing}: Analysis state saved periodically for resumption
\item \textbf{Parallel computation}: Multi-threaded embedding generation and metric calculation
\item \textbf{Visualization}: Automated generation of ensemble plots, correlation matrices, and phase detection comparisons
\end{itemize}

All code is available at [repository URL] with documentation and tests.

\section{Results}

\subsection{The Global-Local Dichotomy}

Our analysis reveals a striking pattern: geometric signatures of conversation exhibit high consistency at global scales but extreme variability at local scales. This dichotomy appears across all model comparisons and conversation types.

\subsubsection{Global Geometric Consistency}

Visual inspection of distance matrices, trajectories, and density evolution reveals remarkable similarities across models. Quantitative analysis confirms these observations:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Model Pair & Distance Matrix $\rho$ & Trajectory $\rho$ \\
\midrule
Within transformers & \transformerInternalCorr{} & 0.693 \\
Within classical & \classicalInternalCorr{} & 0.888 \\
Cross-paradigm (mean) & \crossParadigmCorr{} & 0.427 \\
\midrule
MiniLM-L6 vs MPNet & \miniLMmpnetCorr{} & 0.731 \\
MiniLM-L6 vs Word2Vec & \miniLMwordTovecCorr{} & 0.452 \\
Word2Vec vs GloVe & \wordTovecGloveCorr{} & 0.888 \\
\midrule
Overall range & \distanceCorrRange{} & [0.163, 0.946] \\
\bottomrule
\end{tabular}
\caption{Correlation coefficients for global geometric signatures across model pairs. Note the consistently high correlations, especially for distance matrices.}
\label{tab:correlations}
\end{table}

These correlations far exceed null model baselines (mean $\rho$ = \nullBaselineCorr{}, p < \nullModelPValue{}), indicating genuine structural consistency rather than chance agreement.

\subsubsection{Local Feature Variability}

In stark contrast to global consistency, local phase detection shows extreme variability:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Conversation Type & F1 Range & Agreement $\rho$ Range & Success Rate \\
\midrule
Clear transitions & [0.20, 0.36] & [0.35, 0.76] & 68\% \\
Gradual evolution & [0.08, 0.19] & [-0.14, 0.24] & 23\% \\
Complex dynamics & [0.09, 0.24] & [-0.08, 0.50] & 41\% \\
\bottomrule
\end{tabular}
\caption{Phase detection performance varies dramatically based on conversation characteristics. Success rate indicates percentage of conversations with F1 > 0.15.}
\label{tab:phase_performance}
\end{table}

Individual conversation analysis reveals the pattern clearly: conversations like "Consciousness\_Exploration\_2025-06-12\_7-Y" show excellent phase agreement across models (mean correlation 0.71), while others like "experiment-consciousness-exploration-2025-07-29-2" show complete disagreement (correlations near 0).

\subsection{Cross-Paradigm Invariance}

The most surprising finding is that the global-local dichotomy persists even across fundamentally different embedding paradigms:

\begin{itemize}
\item \textbf{Global agreement}: Transformer and classical models show substantial distance matrix correlations (mean \crossParadigmCorr{})
\item \textbf{Local disagreement}: Phase detection agreement between paradigms is no better than chance for 62\% of conversations
\item \textbf{Visual consistency}: Despite local disagreement, trajectory shapes and density patterns remain visually recognizable
\end{itemize}

This suggests that different architectures capture similar macro-structure while processing micro-structure differently.

\subsection{Statistical Validation}

Hierarchical hypothesis testing reveals an important paradox in our data. While all \tiersPassed{} tiers passed statistical tests:

\begin{enumerate}
\item \textbf{Tier 1 (Within-paradigm)}: Transformer models $\rho > 0.75$ ($p = 2.86 \times 10^{-8}$), Classical models $\rho > 0.70$ ($p < 0.001$)
\item \textbf{Tier 2 (Cross-paradigm)}: Correlations $\rho > 0.50$ ($p = 1.45 \times 10^{-10}$)
\item \textbf{Tier 3 (Hierarchy)}: Within > Cross > Random ordering confirmed via Kruskal-Wallis test ($p < 0.001$, $\eta^2 = 0.65$)
\end{enumerate}

The mean effect size of only \meanEffectSize{} and mean invariance score of \meanInvarianceScore{} (median \medianInvarianceScore{}) reveal that these statistical tests capture global patterns while masking the severe local variability we observe. This highlights how aggregate statistics can obscure important heterogeneity in the data.

Control hypotheses all passed ($p < 0.001$), confirming that patterns are not artifacts of message length or scrambling. Power analysis indicated adequate power (> 0.80) for all primary hypotheses given our sample size.

\section{Discussion}

\subsection{Interpreting the Dichotomy}

The sharp distinction between global consistency and local variability suggests several interpretations:

\textbf{Scale-dependent representation}: Conversations may have hierarchical structure where large-scale patterns (topic flow, emotional arc) are robustly encoded while fine-grained details (precise transition points) depend on model-specific features.

\textbf{Signal vs. noise}: Global patterns may represent the true signal of conversational structure, while local variations reflect model-specific noise or different valid interpretations of ambiguous boundaries.

\textbf{Complementary perspectives}: Different models may capture complementary aspects of conversation, agreeing on overall structure while emphasizing different local features.

\subsection{The Phase Detection Challenge}

Our results reveal phase detection as surprisingly difficult. Even with ensemble methods and statistical corrections, agreement between models remains poor for most conversations. This challenges assumptions about the universality of conversational phases and suggests:

\begin{itemize}
\item Phase boundaries may be inherently ambiguous rather than sharp
\item Different models may legitimately identify different aspects of transitions
\item Human-annotated phases may not correspond to computationally natural boundaries
\end{itemize}

The high variability in phase detection performance (some conversations achieving F1 > 0.30 while others fail completely) indicates that conversation type matters enormously. Structured debates show clearer phases than exploratory discussions.

\subsection{Implications for Geometric Analysis}

Our findings support using geometric methods for conversational analysis, but with important caveats:

\begin{enumerate}
\item \textbf{Macro-analysis is robust}: Overall trajectory shapes, distance relationships, and density evolution provide reliable signals
\item \textbf{Micro-analysis requires caution}: Fine-grained features like exact phase boundaries should not be over-interpreted
\item \textbf{Ensemble methods help but don't solve the problem}: Multiple models provide complementary views but don't guarantee agreement on local features
\end{enumerate}

\subsection{Connections to Behavioral Dynamics}

Intriguingly, the conversations showing clearest geometric structure (high cross-model agreement) correspond to those exhibiting the strongest behavioral phenomena in our prior work:
\begin{itemize}
\item Peer pressure cascades occur in geometrically coherent conversations
\item Breakdown events correspond to geometric discontinuities visible across models
\item Recovery patterns involve returns to earlier embedding regions
\end{itemize}

This suggests geometric coherence may be prerequisite for complex social dynamics.

\section{Limitations}

\begin{itemize}
\item \textbf{Domain specificity}: All conversations focused on consciousness discussions
\item \textbf{Embedding selection}: While diverse, our five models don't exhaust all approaches
\item \textbf{Ground truth ambiguity}: Human phase annotations may not represent objective truth
\item \textbf{Metric dependency}: Results may vary with different similarity metrics
\end{itemize}

\section{Future Directions}

Our findings open several research avenues:

\begin{enumerate}
\item Developing scale-aware methods that separately analyze global and local structure
\item Investigating what makes some conversations more geometrically coherent
\item Testing whether geometric coherence predicts conversational outcomes
\item Exploring alternative definitions of conversational phases
\item Extending analysis to human conversations
\end{enumerate}

\section{Conclusion}

We have presented empirical evidence for a fundamental dichotomy in conversational geometry: remarkable consistency at global scales coupled with surprising variability at local scales. This pattern, persistent across fundamentally different embedding approaches, suggests that conversations possess robust macro-structure while micro-structure remains model-dependent.

The paradox in our findings is instructive: while hierarchical hypothesis testing confirmed "complete geometric invariance" (all tiers passed), the reality is more nuanced. The mean invariance score of \meanInvarianceScore{} tells the true storyâ€”geometric patterns are partially but not completely preserved across models. Our statistical tests detected the strong global signals while the detailed analysis revealed substantial local disagreement.

These findings validate geometric analysis as a tool for understanding conversation, but with important qualifications. Researchers should focus on large-scale patterns rather than fine-grained details, use ensemble methods to identify robust features, and recognize that some aspects of conversation may have multiple valid interpretations.

The global-local dichotomy we observe may reflect a deeper truth about conversation: while overall flow and structure follow universal patterns, the precise boundaries and transitions remain fluid, context-dependent, and open to interpretation. This flexibility may be a feature, not a bug, allowing conversations to adapt to participants and contexts while maintaining coherent structure.

Our work establishes both the promise and limitations of geometric approaches to conversation. Just as physicists distinguish between macroscopic and microscopic phenomena, conversation researchers may need to develop scale-dependent theories that respect the different levels of structure in human dialogue.

\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix

\section{Supplementary Analysis}

\subsection{Conversation-Specific Patterns}

Analysis of individual conversations reveals systematic patterns in geometric coherence:

\begin{itemize}
\item \textbf{High coherence conversations} (top 20\%): Clear topic progression, explicit transitions, structured argumentation
\item \textbf{Low coherence conversations} (bottom 20\%): Exploratory discussion, gradual shifts, recursive themes
\item \textbf{Correlation with length}: Longer conversations show lower phase agreement ($r = -0.31, p < 0.01$)
\end{itemize}

\subsection{Model-Specific Behaviors}

Despite overall consistency, models show characteristic differences:

\begin{itemize}
\item \textbf{Transformer models}: More sensitive to syntactic transitions
\item \textbf{Classical models}: Better at capturing semantic shifts
\item \textbf{Dimensionality effects}: Higher-dimensional models (MPNet) show more nuanced phase detection
\end{itemize}

\section{Statistical Methods}

\subsection{Bootstrap Procedure}

Confidence intervals computed using stratified bootstrap with \bootstrapIterations{} iterations, ensuring balanced representation across conversation types.

\subsection{Multiple Comparison Corrections}

Hierarchical hypothesis testing used Bonferroni correction for Tier 1, FDR correction for Tier 2, and effect size criteria for Tier 3.

\subsection{Null Model Construction}

Three null models tested:
\begin{enumerate}
\item Shuffled message order within conversations
\item Random sampling from message corpus
\item Random walks in embedding space
\end{enumerate}

All showed significantly lower correlations than observed data (p < \nullModelPValue{}).

\end{document}